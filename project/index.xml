<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects on KAILI ZHAO</title>
    <link>https://zkl20061823.github.io/project/</link>
    <description>Recent content in Projects on KAILI ZHAO</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 01 Sep 2019 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="https://zkl20061823.github.io/project/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Pedestrian detection</title>
      <link>https://zkl20061823.github.io/project/ped-project/</link>
      <pubDate>Sun, 01 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://zkl20061823.github.io/project/ped-project/</guid>
      <description>&lt;p&gt;Rarely occluded pedestrians:




  
  





  





  


&lt;video controls &gt;
  &lt;source src=&#34;https://zkl20061823.github.io/img/pedet_video.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;&lt;/p&gt;

&lt;p&gt;Heavily occluded pedestrians:
&lt;img src=&#34;ped1.jpg&#34; width=&#34;2048px&#34; height=&#34;1024px&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This project built a single-stage detector for occluded pedestrian.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Crowd Counting</title>
      <link>https://zkl20061823.github.io/project/cc-project/</link>
      <pubDate>Mon, 27 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://zkl20061823.github.io/project/cc-project/</guid>
      <description>&lt;p&gt;Examples of our predictions on other domain images (captured from &lt;a href=&#34;https://www.youtube.com/watch?v=6NLe4syTWgQ&#34;&gt;Shibuya crossing&lt;/a&gt;):




  
  





  





  


&lt;video controls &gt;
  &lt;source src=&#34;https://zkl20061823.github.io/img/cc_video.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;&lt;/p&gt;

&lt;p&gt;Examples of our predictions on ShanghaiTech B [1]:
&lt;img src=&#34;cc_33.jpg&#34; width=&#34;2048px&#34; height=&#34;1024px&#34; /&gt;
&lt;img src=&#34;cc_35.jpg&#34; width=&#34;2048px&#34; height=&#34;1024px&#34; /&gt;
&lt;img src=&#34;cc_61.jpg&#34; width=&#34;2048px&#34; height=&#34;1024px&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This project implemented model for counting how many people in conjested crowds.
In the projects, we conducted a light-weight model of 0.82 million parameters and included dilated convolutions to help capture information of various head sizes. In crowds of 200 people, our model achieved absolute error of 10. We trained the model on three public datasets: ShanghaiTech A / B [1], WorldExpo [2], and UCSD [3].&lt;/p&gt;

&lt;p&gt;&lt;font size=&#34;3&#34;&gt;
[1] Zhang, Yingying, et al. &amp;ldquo;Single-image crowd counting via multi-column convolutional neural network.&amp;rdquo; Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.&lt;br /&gt;
[2] Zhang, Cong, et al. &amp;ldquo;Cross-scene crowd counting via deep convolutional neural networks.&amp;rdquo; Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.&lt;br /&gt;
[3] A. B. Chan, Zhang-Sheng John Liang, and N. Vasconcelos. Privacy preserving crowd monitoring: Counting people without people models or tracking. In 2008 IEEE Conference on Computer Vision and Pattern Recognition, 2008
&lt;/font&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Facial Expression Recognition</title>
      <link>https://zkl20061823.github.io/project/fer-project/</link>
      <pubDate>Wed, 27 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>https://zkl20061823.github.io/project/fer-project/</guid>
      <description>&lt;p&gt;Demos of our model performed on images captured from the film &lt;a href=&#34;https://www.youtube.com/watch?v=B8rD68a5LQQ&#34;&gt; &amp;ldquo;King of Comedy&amp;rdquo;&lt;/a&gt; produced by Stephen Chow and previous cuts of &lt;a href=&#34;https://www.youtube.com/watch?v=tiicx0d7yBg&#34;&gt; &amp;ldquo;27 Emotions Every Actor Should Know&amp;rdquo; &lt;/a&gt;.




  
  





  





  


&lt;video controls &gt;
  &lt;source src=&#34;https://zkl20061823.github.io/img/fdat_video.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;




  
  





  





  


&lt;video controls &gt;
  &lt;source src=&#34;https://zkl20061823.github.io/img/fdat_exp2.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;&lt;/p&gt;

&lt;p&gt;This project contains three crucial modules including face detection, tracking, and expression recognition.
We included SSD as a single-shot face detector, used data assosiation as face tracker, and exploited Deep Region Learning [1] as expression clssifier.
Specifically, in Deep Region Learning, we introduced soft-max loss to replace  multi-label loss in [1], and used deep region learning module for updating feature maps for each local region.
In addition, we inlcude WIDER-FACE dataset [2] and RAF expression dataset [3] for tranining face detector and expression classifier separately.&lt;/p&gt;

&lt;p&gt;&lt;font size=&#34;3&#34;&gt;
[1] Kaili Zhao, Wen-Sheng Chu, Honggang Zhang, &amp;ldquo;Deep Region and Multi-label Learning for Facial Action Unit Detection&amp;rdquo;. CVPR 2016.&lt;br /&gt;
[2] Yang, Shuo, et al. &amp;ldquo;Wider face: A face detection benchmark.&amp;rdquo; Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.&lt;br /&gt;
[3] Shan Li, Weihong Deng, JunPing DU, &amp;ldquo;Reliable Crowdsourcing and Deep Locality-Preserving Learning for Expression Recognition in the Wild&amp;rdquo;, CVPR 2017.
&lt;/font&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
